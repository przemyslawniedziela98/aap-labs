{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DBXxErK07vQA"},"source":["<center>\n","    <font size=\"5\"> Zaawansowane Metody Sztucznej Inteligencji<br/>\n","        <small><em>Studia stacjonarne II stopnia 2024/2025</em><br/>Kierunek: Informatyka</small>\n","    </font>\n","</center>\n","<br>\n"]},{"cell_type":"markdown","metadata":{"id":"sdRmFujKXUpi"},"source":["# Laboratorium nr 1.1: Sieci neuronowe - przypomnienie"]},{"cell_type":"markdown","metadata":{"id":"FdSpj9H83wJq"},"source":["### Montowanie Google Drive"]},{"cell_type":"code","metadata":{"id":"Shzp91Bw3vTk"},"source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# uaktualnij poniższą ścieżkę\n","path_nb = r'/content/drive/My Drive/Colab Notebooks/ZMSI_24_25/'\n","sys.path.append(path_nb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O6xS51dbEf53"},"source":["## Perceptron\n","![perceptron](http://torus.uck.pk.edu.pl/~amarsz/images/zmum/neuron2.PNG)![eq](http://torus.uck.pk.edu.pl/~amarsz/images/zmum/n2eq2.PNG)"]},{"cell_type":"markdown","metadata":{"id":"mZf_D4QhaPh4"},"source":["### Uczenie perceptronu\n","Uczenie perceptronu należy do grupy uczenia z nauczycielem i polega na takim doborze wektora wag $w$, aby sygnał wyjściowy neuronu $y$ były najbliżej wartości pożądanej $d$. Najpopularniejszą metodą uczenia perceptronu jest tzw. _reguła perceptronu_, którą można opisać w postaci kilku kroków.\n","\n","Załóżmy, że dysponujemy zbiorem wektorów uczących postaci $\\{x^{(0)}, x^{(1)}, \\ldots, x^{(P)}\\}$ oraz odpowiadającym mu zbiorem wartości pożądanych $\\{d^{(0)}, d^{(1)},\\ldots, d^{(P)}\\}$.\n","\n","#### Reguła perceptronu\n","1. Ustalamy $t = 0$.\n","2. Ustalamy w sposób losowy początkowe wartości wektora wag $w$.\n","3. Prezentujemy na wejścia perceptronu wektor uczący $x^{(t)}$.\n","4. Obliczamy odpowiedź perceptronu $y$ zgodnie z wzorem $y = g\\left(\\sum\\limits_{i=0}^N{w_ix^{(t)}_i}\\right)$, gdzie $g$ to fukcja skoku zwracająca $1$ gdy argument jest większy od $0$, a w przeciwnym wypadku $0$.\n","5. Porównujemy odpowiedź perceptronu $y$ z pożądaną odpowiedzią $d^{(t)}$.\n","6. Modyfikujemy wartości wag według poniższych reguł, parametr $\\eta\\in(0,1)$ to _współczynnuk uczenia_:\n","   - jeśli $y = d^{(t)}$ to wagi pozostają niezmienione,\n","   - jeśli $y = 0$, a $d^{(t)}=1$ to $w_i = w_i + \\eta x_i^{(t)}$,\n","   - jeśli $y = 1$, a $d^{(t)}=0$ to $w_i = w_i - \\eta x_i^{(t)}$.\n","7. Jeżeli warunek zatrzymania nie jest spełniony, to ustalamy $t = t + 1$ i wracamy do kroku 3, w przeciwnym przypadku kończymy algorytm.\n","\n","Wykonanie powyższej procedury dla wszystkich wektorów uczących nazywamy _epoką uczenia_. W przypadku uczenia perceptronu wykonujemy tyle epok, aż wszystkie przykłady uczące będą dobrze sklasyfikowane lub błąd klasyfikacji będzie dostatecznie mały. Stabilność oraz szybkość uczenia tym algorytmem w istotny sposób zależy od doboru\n","parametru $\\eta$. Współczynnik ten dobierany jest najczęściej w sposób empiryczny."]},{"cell_type":"markdown","metadata":{"id":"rUb3wQCxGve_"},"source":["## Ćwiczenie 1:\n","Zaimplementuj model preceptronu w postaci klasy."]},{"cell_type":"code","metadata":{"id":"7nV6uNb8GkkV"},"source":["import numpy as np\n","\n","class Perceptron:\n","    # Inicjalizator, ustawiający atrybut self.w oraz self.b jako wektor losowych wag, n ilość sygnałów wejściowych\n","    def __init__(self, n, bias=True):\n","        pass\n","\n","    # Metoda obliczająca odpowiedz modelu dla zadanego sygnału wejściowego x=[x1,x2,...,xN]\n","    def predict(self, x):\n","        pass\n","\n","\n","    # Metoda uczenia według reguły perceptronu, xx - zbiór danych uczących, d - odpowiedzi,\n","    # eta - współczynnik uczenia,\n","    # tol - tolerancja (czyli jak duży błąd jesteśmy w stanie zaakceptować)\n","    def train(self, xx, d, eta, tol):\n","        pass\n","\n","\n","    # Metoda obliczająca błąd dla danych testowych xx\n","    # zwraca błąd oraz wektor odpowiedzi perceptronu dla danych testowych\n","    def evaluate_test(self, xx, d):\n","        pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Wi3iBrw5wJ_"},"source":["## Ćwiczenie 2:\n","- Stwórz dwa obiekty klasy `Perceptron`.\n","- Wczytaj dane z plików `2D.csv` oraz `3D.csv.`\n","- Pierwszy z perceptronów naucz klasyfikować dane z pliku `2D.csv`, ucz tylko na losowej części danych (np. 80%)\n","- Drugi z perceptronów naucz klasyfikować dane z pliku `3D.csv`, ucz tylko na losowej części danych (np. 80%)\n","- Oba zbiory danych są przykładami problemów liniowo separowalnych, a więc należy uczyć modele tak aby uzyskiwać dla danych uczących błąd równy zero.\n","- Przedstaw rezultaty uczenia na wykresach, odpowiednio 2D lub 3D. Na wykresach powinny znaleźć się dane testowe, tzn. te które nie były wykorzystywane w trakcie uczenia oraz linia (płaszczyzna) rozdzielająca klasy."]},{"cell_type":"code","metadata":{"id":"-xJULzQl5t2K"},"source":["import numpy as np\n","import pandas as pd\n","from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mw0UnJmCb1BI"},"source":["df = pd.read_csv(path_nb+'2D.csv', skiprows=1, delimiter=';', names=['X1', 'X2', 'L'], decimal=',')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"20t8AzWHTbal"},"execution_count":null,"outputs":[]}]}